{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import style\n",
    "import librosa\n",
    "import IPython.display\n",
    "import librosa.display\n",
    "import os\n",
    "import random\n",
    "from matplotlib.pyplot import specgram\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name):\n",
    "    X, sample_rate = librosa.load(file_name)\n",
    "    np.nan_to_num(X)\n",
    "    stft = np.abs(librosa.stft(X))\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0)\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "    mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "    contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
    "    tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)\n",
    "    return mfccs, chroma, mel, contrast, tonnetz\n",
    "\n",
    "\n",
    "def parse_audio_files(parent_dir, sub_dirs, classes, file_ext='*.wav'):\n",
    "    features, labels = np.empty((0, 193)), np.empty(0)\n",
    "    for label, sub_dir in enumerate(sub_dirs):\n",
    "        for fn in glob.glob(os.path.join(parent_dir, sub_dir, file_ext)):\n",
    "            mfccs, chroma, mel, contrast,tonnetz = extract_feature(fn)\n",
    "            ext_features = np.hstack([mfccs,chroma,mel,contrast,tonnetz])\n",
    "            features = np.vstack([features,ext_features])\n",
    "            labels = np.append(labels, classes.get(sub_dir))\n",
    "    return np.array(features), np.array(labels, dtype = np.int)\n",
    "\n",
    "\n",
    "def one_hot_encode(labels):\n",
    "    n_labels = len(labels)\n",
    "    n_unique_labels = len(np.unique(labels))\n",
    "    one_hot_encode = np.zeros((n_labels, n_unique_labels))\n",
    "    one_hot_encode[np.arange(n_labels), labels] = 1\n",
    "    return one_hot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nm', 'pm', 'VS', 'Mala']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_dir = '/mnt/4dd2488f-d1c5-4d59-85f0-86b7c7057817/SPEAKER_MIMICRY/VS_CODE_IMPLI/Train_tsk_02'\n",
    "test_dir = '/mnt/4dd2488f-d1c5-4d59-85f0-86b7c7057817/SPEAKER_MIMICRY/VS_CODE_IMPLI/Test_tsk_02'\n",
    "\n",
    "os.listdir(train_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nm', 'pm', 'VS', 'Mala']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[ 8.0192411e-05 -2.1533678e-04 -4.2327435e-04 ... -3.7651455e-03\n",
      " -4.8132185e-03 -2.3233504e-03] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[-0.00643864 -0.00676405 -0.00463421 ...  0.02510195  0.02140986\n",
      "  0.0250639 ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[ 0.00374938  0.00595315  0.00774857 ... -0.28290266 -0.26409227\n",
      " -0.27941763] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[0.1457217  0.2100989  0.12865467 ... 0.00588224 0.00433644 0.00148834] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[0.00150702 0.00240742 0.00552311 ... 0.01811097 0.01690458 0.01630865] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[-0.00912076 -0.03465854 -0.05059636 ... -0.3498334  -0.40312457\n",
      " -0.42064297] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[0.15320875 0.18684064 0.0639966  ... 0.06718193 0.04256715 0.00815597] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[0.00842168 0.01265621 0.01414792 ... 0.01976204 0.0192205  0.02322465] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[ 0.00283347  0.00463619  0.00394038 ... -0.00561744 -0.00520177\n",
      " -0.00521704] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[0. 0. 0. ... 0. 0. 0.] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[-0.00184445 -0.00221419 -0.00108207 ...  0.04004586  0.05123955\n",
      "  0.05229818] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[0.         0.         0.         ... 0.09297207 0.13229913 0.18280415] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 2.6998241e-05 3.4608947e-05\n",
      " 0.0000000e+00] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[ 0.0236656   0.01452485 -0.02128187 ...  0.15490189  0.14787686\n",
      "  0.08918238] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[ 0.08256298  0.10142423  0.05067803 ... -0.02725292 -0.02234937\n",
      " -0.01831742] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[-0.00308705 -0.00460238 -0.00395666 ...  0.0148781   0.01194972\n",
      "  0.00999224] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[-0.12763645 -0.29362038 -0.3657138  ... -0.02515291  0.00153891\n",
      " -0.05118215] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[ 0.0061965   0.00923032  0.00772226 ... -0.01653729 -0.01664839\n",
      " -0.01857988] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[ 0.04203616  0.06746049  0.05978665 ... -0.02557463 -0.02411282\n",
      " -0.0308702 ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[ 6.4888358e-05 -7.8514422e-06 -5.1027979e-05 ...  1.1518572e-02\n",
      "  1.1860581e-02  1.2787228e-02] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[-0.02026014 -0.02471127 -0.00763548 ... -0.01523731 -0.01474668\n",
      " -0.00861085] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[0.00028874 0.00031769 0.00018284 ... 0.01046877 0.0135619  0.01799425] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[-0.30506837 -0.43318456 -0.31573775 ...  0.6071203   0.584312\n",
      "  0.5500767 ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[-0.04798543 -0.05137672 -0.01789257 ...  0.1521869   0.14868157\n",
      "  0.16711113] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[ 0.03419303  0.03830027  0.01579272 ... -0.01138781 -0.01218945\n",
      " -0.0116373 ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[-0.00484009 -0.00724477 -0.00565618 ...  0.00306044  0.00318254\n",
      "  0.0034744 ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[-0.00439148  0.00463131  0.00426282 ...  0.14057723  0.12519789\n",
      "  0.11019509] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[ 0.0003794   0.00243001  0.00377249 ...  0.01547899  0.00410742\n",
      " -0.02668791] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[-2.8062990e-05 -7.1063655e-04 -8.3416427e-04 ... -3.3560114e-05\n",
      " -3.0949104e-04 -1.5222505e-03] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[-0.0051492  -0.00448339 -0.04265938 ... -0.03689916 -0.02907582\n",
      " -0.02781941] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[-0.1902255  -0.29216883 -0.26542684 ...  0.03816262  0.03012465\n",
      "  0.02162744] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[ 0.0008443   0.00122291  0.00059256 ... -0.00034309 -0.0013028\n",
      " -0.00216333] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[0.00039502 0.00144618 0.00161585 ... 0.17106551 0.16986449 0.1678486 ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:21: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return np.array(features), np.array(labels, dtype = np.int)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sub_dirs = ['nm', 'pm', 'VS', 'Mala']\n",
    "classes = {'nm': 0, 'pm': 1, 'VS': 2, 'Mala': 3}\n",
    "features1, labels1 = parse_audio_files(train_dir, sub_dirs, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = one_hot_encode(labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, x_test, y_train, y_test = train_test_split(features1, one_hot, test_size=0.1, random_state = 127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 193)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-27 20:41:53.763521: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-27 20:41:54.195600: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-27 20:41:54.195639: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-27 20:41:55.525616: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-27 20:41:55.525757: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-27 20:41:55.525771: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-27 20:41:58.018151: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-27 20:41:58.018184: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-27 20:41:58.018207: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cruz-Predator-PH315-51): /proc/driver/nvidia/version does not exist\n",
      "2022-12-27 20:41:58.018462: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "num_labels = one_hot.shape[1]\n",
    "filter_size = 4\n",
    "\n",
    "def build_model_graph(input_shape=(40,)):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_labels))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    # Compile the model\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint \n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 100\n",
    "num_batch_size = 32\n",
    "checkpointer = ModelCheckpoint(filepath=\"dnnfdp.hdf5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 23.8318 - accuracy: 0.2069\n",
      "Epoch 1: val_accuracy improved from -inf to 0.25000, saving model to dnnfdp.hdf5\n",
      "1/1 [==============================] - 1s 1s/step - loss: 23.8318 - accuracy: 0.2069 - val_loss: 5.9117 - val_accuracy: 0.2500\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 26.0210 - accuracy: 0.3103\n",
      "Epoch 2: val_accuracy did not improve from 0.25000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 26.0210 - accuracy: 0.3103 - val_loss: 7.0335 - val_accuracy: 0.2500\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 29.4770 - accuracy: 0.2414\n",
      "Epoch 3: val_accuracy did not improve from 0.25000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 29.4770 - accuracy: 0.2414 - val_loss: 9.1785 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 17.6898 - accuracy: 0.3793\n",
      "Epoch 4: val_accuracy did not improve from 0.25000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 17.6898 - accuracy: 0.3793 - val_loss: 9.9394 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 12.7478 - accuracy: 0.4138\n",
      "Epoch 5: val_accuracy did not improve from 0.25000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 12.7478 - accuracy: 0.4138 - val_loss: 8.5386 - val_accuracy: 0.2500\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 20.5805 - accuracy: 0.2759\n",
      "Epoch 6: val_accuracy did not improve from 0.25000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 20.5805 - accuracy: 0.2759 - val_loss: 6.5576 - val_accuracy: 0.2500\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 19.0626 - accuracy: 0.2759\n",
      "Epoch 7: val_accuracy did not improve from 0.25000\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 19.0626 - accuracy: 0.2759 - val_loss: 5.6754 - val_accuracy: 0.2500\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 14.7033 - accuracy: 0.4483\n",
      "Epoch 8: val_accuracy did not improve from 0.25000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 14.7033 - accuracy: 0.4483 - val_loss: 5.2346 - val_accuracy: 0.2500\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 21.6486 - accuracy: 0.2414\n",
      "Epoch 9: val_accuracy did not improve from 0.25000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 21.6486 - accuracy: 0.2414 - val_loss: 3.8378 - val_accuracy: 0.2500\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 18.1348 - accuracy: 0.3448\n",
      "Epoch 10: val_accuracy improved from 0.25000 to 0.50000, saving model to dnnfdp.hdf5\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 18.1348 - accuracy: 0.3448 - val_loss: 2.7216 - val_accuracy: 0.5000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 16.5536 - accuracy: 0.3448\n",
      "Epoch 11: val_accuracy improved from 0.50000 to 0.75000, saving model to dnnfdp.hdf5\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 16.5536 - accuracy: 0.3448 - val_loss: 2.0473 - val_accuracy: 0.7500\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 17.1301 - accuracy: 0.4483\n",
      "Epoch 12: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 17.1301 - accuracy: 0.4483 - val_loss: 1.5862 - val_accuracy: 0.7500\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 14.8209 - accuracy: 0.3793\n",
      "Epoch 13: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 14.8209 - accuracy: 0.3793 - val_loss: 1.6534 - val_accuracy: 0.7500\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 10.2342 - accuracy: 0.4483\n",
      "Epoch 14: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 10.2342 - accuracy: 0.4483 - val_loss: 1.8046 - val_accuracy: 0.7500\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 10.5548 - accuracy: 0.4483\n",
      "Epoch 15: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 10.5548 - accuracy: 0.4483 - val_loss: 1.8120 - val_accuracy: 0.7500\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 10.8948 - accuracy: 0.4138\n",
      "Epoch 16: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 10.8948 - accuracy: 0.4138 - val_loss: 1.9556 - val_accuracy: 0.7500\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 18.4002 - accuracy: 0.4138\n",
      "Epoch 17: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 18.4002 - accuracy: 0.4138 - val_loss: 2.1165 - val_accuracy: 0.7500\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 7.4660 - accuracy: 0.4483\n",
      "Epoch 18: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7.4660 - accuracy: 0.4483 - val_loss: 2.3422 - val_accuracy: 0.7500\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 10.6413 - accuracy: 0.5172\n",
      "Epoch 19: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 10.6413 - accuracy: 0.5172 - val_loss: 2.5039 - val_accuracy: 0.7500\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 6.3116 - accuracy: 0.6552\n",
      "Epoch 20: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 6.3116 - accuracy: 0.6552 - val_loss: 2.8509 - val_accuracy: 0.7500\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 7.8061 - accuracy: 0.5517\n",
      "Epoch 21: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 7.8061 - accuracy: 0.5517 - val_loss: 3.4512 - val_accuracy: 0.5000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 8.4542 - accuracy: 0.5862\n",
      "Epoch 22: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 8.4542 - accuracy: 0.5862 - val_loss: 4.2786 - val_accuracy: 0.5000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 7.3074 - accuracy: 0.5862\n",
      "Epoch 23: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 7.3074 - accuracy: 0.5862 - val_loss: 5.1588 - val_accuracy: 0.5000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 9.2013 - accuracy: 0.5517\n",
      "Epoch 24: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 9.2013 - accuracy: 0.5517 - val_loss: 5.8637 - val_accuracy: 0.5000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 6.0608 - accuracy: 0.5862\n",
      "Epoch 25: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 6.0608 - accuracy: 0.5862 - val_loss: 6.2267 - val_accuracy: 0.5000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 10.4528 - accuracy: 0.4138\n",
      "Epoch 26: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 10.4528 - accuracy: 0.4138 - val_loss: 6.2183 - val_accuracy: 0.5000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 12.0214 - accuracy: 0.4828\n",
      "Epoch 27: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 12.0214 - accuracy: 0.4828 - val_loss: 5.8830 - val_accuracy: 0.5000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 7.1920 - accuracy: 0.6207\n",
      "Epoch 28: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 7.1920 - accuracy: 0.6207 - val_loss: 5.3741 - val_accuracy: 0.5000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 5.9258 - accuracy: 0.5862\n",
      "Epoch 29: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 5.9258 - accuracy: 0.5862 - val_loss: 4.5581 - val_accuracy: 0.5000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 7.5547 - accuracy: 0.5172\n",
      "Epoch 30: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 7.5547 - accuracy: 0.5172 - val_loss: 3.8703 - val_accuracy: 0.5000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 7.4987 - accuracy: 0.4483\n",
      "Epoch 31: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.4987 - accuracy: 0.4483 - val_loss: 3.5303 - val_accuracy: 0.7500\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 5.5551 - accuracy: 0.5517\n",
      "Epoch 32: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 5.5551 - accuracy: 0.5517 - val_loss: 3.3928 - val_accuracy: 0.5000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.2128 - accuracy: 0.6552\n",
      "Epoch 33: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 4.2128 - accuracy: 0.6552 - val_loss: 3.4260 - val_accuracy: 0.5000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.9295 - accuracy: 0.6897\n",
      "Epoch 34: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.9295 - accuracy: 0.6897 - val_loss: 3.4033 - val_accuracy: 0.5000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.4890 - accuracy: 0.6552\n",
      "Epoch 35: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 4.4890 - accuracy: 0.6552 - val_loss: 3.4528 - val_accuracy: 0.5000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 5.3338 - accuracy: 0.6897\n",
      "Epoch 36: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 5.3338 - accuracy: 0.6897 - val_loss: 3.3987 - val_accuracy: 0.5000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 5.7007 - accuracy: 0.5517\n",
      "Epoch 37: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 5.7007 - accuracy: 0.5517 - val_loss: 3.3766 - val_accuracy: 0.5000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.3272 - accuracy: 0.6552\n",
      "Epoch 38: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.3272 - accuracy: 0.6552 - val_loss: 3.1883 - val_accuracy: 0.5000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 5.1445 - accuracy: 0.5862\n",
      "Epoch 39: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 5.1445 - accuracy: 0.5862 - val_loss: 3.0968 - val_accuracy: 0.5000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.4821 - accuracy: 0.6552\n",
      "Epoch 40: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.4821 - accuracy: 0.6552 - val_loss: 3.0462 - val_accuracy: 0.5000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.6189 - accuracy: 0.6552\n",
      "Epoch 41: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6189 - accuracy: 0.6552 - val_loss: 2.9026 - val_accuracy: 0.5000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 5.0931 - accuracy: 0.5862\n",
      "Epoch 42: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 5.0931 - accuracy: 0.5862 - val_loss: 2.8267 - val_accuracy: 0.5000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.1368 - accuracy: 0.6897\n",
      "Epoch 43: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.1368 - accuracy: 0.6897 - val_loss: 2.7884 - val_accuracy: 0.5000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.9244 - accuracy: 0.7241\n",
      "Epoch 44: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.9244 - accuracy: 0.7241 - val_loss: 2.8286 - val_accuracy: 0.5000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.6540 - accuracy: 0.7586\n",
      "Epoch 45: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.6540 - accuracy: 0.7586 - val_loss: 2.8416 - val_accuracy: 0.5000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.4115 - accuracy: 0.6207\n",
      "Epoch 46: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.4115 - accuracy: 0.6207 - val_loss: 2.8794 - val_accuracy: 0.5000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.6137 - accuracy: 0.6552\n",
      "Epoch 47: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.6137 - accuracy: 0.6552 - val_loss: 2.7414 - val_accuracy: 0.5000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5892 - accuracy: 0.7241\n",
      "Epoch 48: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.5892 - accuracy: 0.7241 - val_loss: 2.7423 - val_accuracy: 0.5000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7220 - accuracy: 0.8621\n",
      "Epoch 49: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.7220 - accuracy: 0.8621 - val_loss: 2.7790 - val_accuracy: 0.5000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.3968 - accuracy: 0.7241\n",
      "Epoch 50: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.3968 - accuracy: 0.7241 - val_loss: 2.7768 - val_accuracy: 0.5000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.0821 - accuracy: 0.6897\n",
      "Epoch 51: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 4.0821 - accuracy: 0.6897 - val_loss: 2.7888 - val_accuracy: 0.5000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.0603 - accuracy: 0.8276\n",
      "Epoch 52: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.0603 - accuracy: 0.8276 - val_loss: 2.9146 - val_accuracy: 0.5000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7983 - accuracy: 0.7931\n",
      "Epoch 53: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.7983 - accuracy: 0.7931 - val_loss: 3.0267 - val_accuracy: 0.5000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2863 - accuracy: 0.7586\n",
      "Epoch 54: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.2863 - accuracy: 0.7586 - val_loss: 3.1219 - val_accuracy: 0.5000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3591 - accuracy: 0.7586\n",
      "Epoch 55: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.3591 - accuracy: 0.7586 - val_loss: 3.2115 - val_accuracy: 0.5000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6963 - accuracy: 0.7931\n",
      "Epoch 56: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.6963 - accuracy: 0.7931 - val_loss: 3.1879 - val_accuracy: 0.5000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.0135 - accuracy: 0.5517\n",
      "Epoch 57: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.0135 - accuracy: 0.5517 - val_loss: 3.1965 - val_accuracy: 0.5000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.5006 - accuracy: 0.7241\n",
      "Epoch 58: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.5006 - accuracy: 0.7241 - val_loss: 3.3722 - val_accuracy: 0.5000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.9377 - accuracy: 0.7241\n",
      "Epoch 59: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.9377 - accuracy: 0.7241 - val_loss: 3.5906 - val_accuracy: 0.5000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.7033 - accuracy: 0.5862\n",
      "Epoch 60: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7033 - accuracy: 0.5862 - val_loss: 3.7857 - val_accuracy: 0.5000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9264 - accuracy: 0.6207\n",
      "Epoch 61: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.9264 - accuracy: 0.6207 - val_loss: 3.8196 - val_accuracy: 0.5000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1470 - accuracy: 0.7931\n",
      "Epoch 62: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.1470 - accuracy: 0.7931 - val_loss: 3.8887 - val_accuracy: 0.5000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2339 - accuracy: 0.6897\n",
      "Epoch 63: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.2339 - accuracy: 0.6897 - val_loss: 3.9409 - val_accuracy: 0.5000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3024 - accuracy: 0.8276\n",
      "Epoch 64: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.3024 - accuracy: 0.8276 - val_loss: 3.9601 - val_accuracy: 0.5000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.1115 - accuracy: 0.6897\n",
      "Epoch 65: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.1115 - accuracy: 0.6897 - val_loss: 4.0199 - val_accuracy: 0.5000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2816 - accuracy: 0.7241\n",
      "Epoch 66: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.2816 - accuracy: 0.7241 - val_loss: 4.0885 - val_accuracy: 0.5000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6667 - accuracy: 0.8276\n",
      "Epoch 67: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.6667 - accuracy: 0.8276 - val_loss: 4.2619 - val_accuracy: 0.5000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3619 - accuracy: 0.8621\n",
      "Epoch 68: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.3619 - accuracy: 0.8621 - val_loss: 4.4324 - val_accuracy: 0.5000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.8319 - accuracy: 0.7241\n",
      "Epoch 69: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.8319 - accuracy: 0.7241 - val_loss: 4.3839 - val_accuracy: 0.5000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.0687 - accuracy: 0.7586\n",
      "Epoch 70: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.0687 - accuracy: 0.7586 - val_loss: 4.2183 - val_accuracy: 0.5000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9866 - accuracy: 0.7586\n",
      "Epoch 71: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.9866 - accuracy: 0.7586 - val_loss: 4.0578 - val_accuracy: 0.5000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5799 - accuracy: 0.8276\n",
      "Epoch 72: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.5799 - accuracy: 0.8276 - val_loss: 3.9001 - val_accuracy: 0.5000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1566 - accuracy: 0.8276\n",
      "Epoch 73: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.1566 - accuracy: 0.8276 - val_loss: 3.7480 - val_accuracy: 0.5000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4508 - accuracy: 0.8966\n",
      "Epoch 74: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.4508 - accuracy: 0.8966 - val_loss: 3.6757 - val_accuracy: 0.5000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9982 - accuracy: 0.7931\n",
      "Epoch 75: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.9982 - accuracy: 0.7931 - val_loss: 3.5744 - val_accuracy: 0.5000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6250 - accuracy: 0.8621\n",
      "Epoch 76: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.6250 - accuracy: 0.8621 - val_loss: 3.4612 - val_accuracy: 0.5000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5136 - accuracy: 0.8621\n",
      "Epoch 77: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.5136 - accuracy: 0.8621 - val_loss: 3.2978 - val_accuracy: 0.5000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7566 - accuracy: 0.8276\n",
      "Epoch 78: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.7566 - accuracy: 0.8276 - val_loss: 3.1546 - val_accuracy: 0.5000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8913 - accuracy: 0.8966\n",
      "Epoch 79: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.8913 - accuracy: 0.8966 - val_loss: 2.9947 - val_accuracy: 0.5000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3788 - accuracy: 0.8276\n",
      "Epoch 80: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3788 - accuracy: 0.8276 - val_loss: 2.8768 - val_accuracy: 0.5000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7176 - accuracy: 0.7931\n",
      "Epoch 81: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.7176 - accuracy: 0.7931 - val_loss: 2.7941 - val_accuracy: 0.5000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3102 - accuracy: 0.8276\n",
      "Epoch 82: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.3102 - accuracy: 0.8276 - val_loss: 2.6711 - val_accuracy: 0.5000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8395 - accuracy: 0.8621\n",
      "Epoch 83: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.8395 - accuracy: 0.8621 - val_loss: 2.6434 - val_accuracy: 0.5000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4419 - accuracy: 0.9310\n",
      "Epoch 84: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4419 - accuracy: 0.9310 - val_loss: 2.6247 - val_accuracy: 0.5000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.8104 - accuracy: 0.8621\n",
      "Epoch 85: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.8104 - accuracy: 0.8621 - val_loss: 2.7211 - val_accuracy: 0.5000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6152 - accuracy: 0.8276\n",
      "Epoch 86: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.6152 - accuracy: 0.8276 - val_loss: 2.8111 - val_accuracy: 0.5000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1877 - accuracy: 0.8276\n",
      "Epoch 87: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.1877 - accuracy: 0.8276 - val_loss: 2.9338 - val_accuracy: 0.5000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3040 - accuracy: 0.8966\n",
      "Epoch 88: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3040 - accuracy: 0.8966 - val_loss: 3.1472 - val_accuracy: 0.5000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0522 - accuracy: 0.8621\n",
      "Epoch 89: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.0522 - accuracy: 0.8621 - val_loss: 3.4125 - val_accuracy: 0.5000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5157 - accuracy: 0.8966\n",
      "Epoch 90: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5157 - accuracy: 0.8966 - val_loss: 3.7101 - val_accuracy: 0.5000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0363 - accuracy: 0.8966\n",
      "Epoch 91: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.0363 - accuracy: 0.8966 - val_loss: 4.0036 - val_accuracy: 0.5000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5267 - accuracy: 0.8966\n",
      "Epoch 92: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5267 - accuracy: 0.8966 - val_loss: 4.2856 - val_accuracy: 0.5000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8721 - accuracy: 0.8276\n",
      "Epoch 93: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.8721 - accuracy: 0.8276 - val_loss: 4.5087 - val_accuracy: 0.5000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8887 - accuracy: 0.8621\n",
      "Epoch 94: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.8887 - accuracy: 0.8621 - val_loss: 4.7036 - val_accuracy: 0.5000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7993 - accuracy: 0.8966\n",
      "Epoch 95: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.7993 - accuracy: 0.8966 - val_loss: 4.8726 - val_accuracy: 0.5000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3292 - accuracy: 0.8276\n",
      "Epoch 96: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3292 - accuracy: 0.8276 - val_loss: 5.0237 - val_accuracy: 0.5000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8773 - accuracy: 0.8966\n",
      "Epoch 97: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.8773 - accuracy: 0.8966 - val_loss: 5.1527 - val_accuracy: 0.5000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4735 - accuracy: 0.9655\n",
      "Epoch 98: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.4735 - accuracy: 0.9655 - val_loss: 5.2692 - val_accuracy: 0.5000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9804 - accuracy: 0.8276\n",
      "Epoch 99: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.9804 - accuracy: 0.8276 - val_loss: 5.3523 - val_accuracy: 0.5000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2280 - accuracy: 0.9310\n",
      "Epoch 100: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.2280 - accuracy: 0.9310 - val_loss: 5.4323 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fec545ec4c0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test),callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: {0:.2%}\".format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[ 0.000000e+00  0.000000e+00  0.000000e+00 ... -9.254414e-07  2.067170e-06\n",
      "  0.000000e+00] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[-0.00119755 -0.00084924 -0.00064206 ...  0.00080524  0.00048259\n",
      "  0.0002059 ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[ 0.          0.          0.         ... -0.00047063 -0.00054746\n",
      " -0.00040489] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[ 1.4644882e-06 -2.3482004e-04  1.5982481e-04 ...  8.6311074e-03\n",
      "  6.7476803e-03  0.0000000e+00] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[-0.00017221 -0.00020961 -0.00016796 ...  0.00043707  0.00063345\n",
      "  0.        ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[ 0.          0.          0.         ... -0.00358758  0.00166401\n",
      " -0.00675077] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  2.9179559e-05\n",
      " -2.2510408e-06  0.0000000e+00] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[0.00941668 0.0106774  0.00856549 ... 0.00829556 0.00901159 0.        ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ... -6.0814295e-05\n",
      " -6.0449875e-05 -6.3622821e-05] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[ 6.8550762e-05 -1.6393598e-04 -3.1922487e-04 ... -3.6955557e-06\n",
      "  1.7561123e-04  0.0000000e+00] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[0. 0. 0. ... 0. 0. 0.] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[ 0.09698916  0.11537904  0.0615954  ... -0.00177136 -0.00034354\n",
      "  0.00113189] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[-0.04798543 -0.05137672 -0.01789257 ...  0.1534463   0.11657579\n",
      "  0.07751188] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[-0.00178093 -0.0012854  -0.00066504 ... -0.00105677 -0.00088943\n",
      "  0.        ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[ 0.          0.          0.         ... -0.00126983 -0.00098932\n",
      "  0.        ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[ 0.00012112  0.00023448  0.00019534 ... -0.00510969 -0.00490376\n",
      "  0.        ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[ 0.000000e+00  0.000000e+00  0.000000e+00 ... -3.073550e-05 -6.828008e-05\n",
      "  0.000000e+00] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ... -9.9419367e-06\n",
      " -3.4819273e-06  0.0000000e+00] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[0.0025286  0.00292597 0.00241005 ... 0.00217773 0.00136345 0.        ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[0.000000e+00 0.000000e+00 0.000000e+00 ... 5.826955e-05 4.884305e-06\n",
      " 0.000000e+00] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[ 0.          0.          0.         ... -0.00204053 -0.00279046\n",
      " -0.00194175] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[0.         0.         0.         ... 0.00075269 0.00050043 0.00024575] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ... -2.3682844e-05\n",
      " -5.6885085e-05  0.0000000e+00] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[ 3.0945437e-05 -1.3458582e-04  1.2354496e-05 ...  5.4213069e-03\n",
      "  5.7068798e-03  0.0000000e+00] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ... -2.1758084e-05\n",
      " -2.2499329e-08  0.0000000e+00] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:7: FutureWarning: Pass y=[ 0.          0.          0.         ... -0.00017369 -0.00019876\n",
      "  0.        ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
      "/tmp/ipykernel_29035/2441616808.py:21: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return np.array(features), np.array(labels, dtype = np.int)\n"
     ]
    }
   ],
   "source": [
    "sub_dirs = ['nm', 'pm', 'VS', 'Mala']\n",
    "classes = {'nm': 0, 'pm': 1, 'VS': 2, 'Mala': 3}\n",
    "features, labels = parse_audio_files(test_dir, sub_dirs, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.85496521e+02,  7.05440826e+01, -8.69348049e+00, ...,\n",
       "         1.53890626e-02,  5.34147028e-02, -5.91246754e-02],\n",
       "       [-4.03394623e+02,  1.18806412e+02, -3.02552605e+01, ...,\n",
       "        -3.33223307e-02, -7.13518584e-04,  1.17023634e-02],\n",
       "       [-2.10548889e+02,  8.95357056e+01, -1.60869007e+01, ...,\n",
       "        -8.04908113e-03,  4.00257375e-04, -4.36154907e-03],\n",
       "       ...,\n",
       "       [-3.58777191e+02,  1.03966736e+02, -3.27147484e+00, ...,\n",
       "         6.30983289e-02,  1.09606349e-02, -2.65028507e-02],\n",
       "       [-2.65419373e+02,  8.49839859e+01, -1.28896999e+01, ...,\n",
       "         2.38555490e-03,  5.11086843e-03,  1.81329763e-03],\n",
       "       [-4.45652344e+02,  9.34927368e+01, -8.49128342e+00, ...,\n",
       "         4.73379586e-03,  5.81958999e-03, -5.17249304e-03]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3]\n",
      "[2, 2, 0, 0, 0, 1, 2, 0, 2, 1, 2, 2, 2, 2, 0, 1, 1, 2, 2, 2, 0, 0, 2, 1, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "X1 = features\n",
    "y1 = labels\n",
    "# print(X1)\n",
    "print(len(X1))\n",
    "\n",
    "ynew = model.predict(X1)\n",
    "# show the inputs and predicted probabilities\n",
    "pred_label = []\n",
    "original=[]\n",
    "for i in range(len(X1)):\n",
    "  Predicted=ynew[i]\n",
    "  # print(Predicted)\n",
    "  # # print(\"X=%s,\\n Predicted=%s\" % (X1[i], ynew[i]))\n",
    "  pred_final1 = np.argmax(Predicted)\n",
    "  pred_label.append(pred_final1)\n",
    "  y1 = labels\n",
    "\n",
    "print(labels)\n",
    "print(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: xlabel='Predicted', ylabel='Actual'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "y_label = [\"vs\", \"mala\", \"pm\",\"nm\"]\n",
    "\n",
    "cm=confusion_matrix(labels,pred_label)\n",
    "df_cm = pd.DataFrame(cm, columns=np.unique(y_label), index = np.unique(y_label))\n",
    "df_cm.index.name = 'Actual'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "plt.figure(figsize = (10,4))\n",
    "sn.set(font_scale=1.4)#for label size\n",
    "sn.heatmap(df_cm, cmap=\"Greens\", annot=True,annot_kws={\"size\": 16})# font size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.375      0.4        0.30769231        nan]\n",
      "[0.6        0.4        0.66666667 0.        ]\n",
      "[0.46153846 0.4        0.42105263        nan]\n",
      "[0.73076923 0.76923077 0.57692308 0.61538462]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29035/1529887052.py:18: RuntimeWarning: invalid value encountered in divide\n",
      "  precision = TP/(TP+FP)\n"
     ]
    }
   ],
   "source": [
    "TP = np.diag(cm)\n",
    "# print(TP)\n",
    "FP = np.sum(cm, axis=0) - TP\n",
    "# print(FP)\n",
    "FN = np.sum(cm, axis=1) - TP\n",
    "# print(FN)\n",
    "num_classes = 4\n",
    "TN = []\n",
    "for i in range(num_classes):\n",
    "    temp = np.delete(cm, i, 0)    # delete ith row\n",
    "    temp = np.delete(temp, i, 1)  # delete ith column\n",
    "    TN.append(sum(sum(temp)))\n",
    "# print(TN)\n",
    "# l = 10000\n",
    "# total\n",
    "# for i in range(num_classes):\n",
    "#     print(TP[i] + FP[i] + FN[i] + TN[i])\n",
    "precision = TP/(TP+FP)\n",
    "print(precision)\n",
    "recall = TP/(TP+FN)\n",
    "print(recall)\n",
    "# specificity = TN/(TN+FP)\n",
    "# print(specificity)\n",
    "F1score= 2*((precision*recall)/(precision+recall))\n",
    "print(F1score)\n",
    "test_accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "041ad26dbb10a8661e7450419833b1f895b9404433ce6ce1546434e658537640"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
